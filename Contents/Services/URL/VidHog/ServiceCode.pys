import re, urlparse, cgi, time, urllib, urllib2from BeautifulSoup import BeautifulSoupUSER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/534.51.22 (KHTML, like Gecko) Version/5.1.1 Safari/534.51.22'def NormalizeURL(url):	#Log("*********** In VidhHog normalizeURL")		# Deal with special providerInfo URL built up by plugin to return	# info about this provider. For all other normal URLs, do nothing. 	if ("providerinfo" in url):			try:			show = Prefs["show_vidhog"]		except Exception, ex:			show = True				if (show):			return url + "&supported=true"		else:			return url				else:		return url	def MetadataObjectForURL(url): 	#Log('In MetadataObjectForURL for VidHog (' + url + ')')		video = VideoClipObject(		title = 'VidHog Redirect Page',		summary = 'VidHog Redirect Page',		thumb = None,	)		return video	def MediaObjectsForURL(url):	# Page flow from start to video is: 	#	- Initial Page	#	- Countdown Page	#	- Video Link Page	#	# This plugin treats the pages as a two step process where each step can be carried	# out indepedently or one after the other. 	# 	# - To run this a single step, pass in the video URL.	# - To stop after the timer has been initialised and return the URL of the video link page, 	#   append nowait=true to the query string.	# - To retrieve the video after the timer has elapsed, pass in the URL returned by calling this	#   function with nowait=true.		#Log("*********************************************************************")		stage1 = True	stage2 = True		# Work out what parts of the process we need to run.	if ("op=download2" in url):			# Only intersted in second part of process as passed in URL is for Video Link page.		# Note that it's expected that the correct delay has been observed by whoever's passing		# in this URL.		stage1 = False		stage2_URL = url			if ("nowait=true" in url):			# Caller doesn't want us to wait if countdown page has a delay.		stage2 = False				# Remove arg from url.		url_parts = urlparse.urlparse(url)		url_parts =  urlparse.ParseResult(url_parts.scheme, url_parts.netloc, url_parts.path, "", "", "")		url = url_parts.geturl()				if (stage1):			# Deal with initial page.		try:			request = urllib2.Request(url)			request.add_header('User-agent', USER_AGENT)			response = urllib2.urlopen(request)					#Log("Requesting: " + url)			soup = BeautifulSoup(response.read())					except Exception, ex:			return LogProviderError("Error whilst retrieving initial provider page (" + url + ")", ex)					#Log(str(soup))				# Look for any errors from provider.		errors = soup.find('font',{ 'class':'err'})		if (errors is not None):			return LogProviderError("Provider reachable but has returned following error: " + errors.string)				# Extract out these form elements...		formElems = ['op', 'id', 'fname', 'method_free', 'referer', 'usr_login']		params = {}			try:			for formElem in formElems:				formElemVal =  soup.find('input', {'name' : formElem })['value']				params[formElem] = formElemVal		except Exception, ex:			return LogProviderError("Error whilst retrieving information to go from intial page to countdown page", ex)				# Submit form with extracted elements.		headers = { 'Referer': url }				try:			soup = BeautifulSoup(HTTP.Request(url,values=params,headers=headers).content)		except Exception, ex:			return LogProviderError("Error whilst retrieving countdown page (" + url + ")", ex)					# Deal with the timer page.		# Extract out these form elements...		formElems = ['down_direct', 'id', 'method_free', 'method_premium', 'op', 'rand', 'referer']		params = {}			try:			for formElem in formElems:				formElemVal =  soup.find('input', {'name' : formElem })['value']				params[formElem] = formElemVal		except Exception, ex:			return LogProviderError("Error whilst retrieving information to go from countdown page to download page", ex)				#Log("*********************************************************************")		#Log("*********************************************************************")		#Log("Params:" + str(params))			# Extract out delay to wait.		try:			delay = int(soup.find('span', id='countdown_str').span.string)		except (Exception, ex):			return LogProviderError("Delay not found on download page. Has something changed?", ex)					#Log("Delay: " + str(delay))			# Create a stage 2 URL. This will be broken down again for stage 2.		stage2_URL = url + "?" + urllib.urlencode(params)				#Log("Stage 2 url: " + stage2_URL)		if (not stage2):				ret = []			ret.append(				MediaObject(					parts = [PartObject(key=stage2_URL, duration=delay)],				)			)			  		return ret	  			  	else:	  				# Looks like the page is happy for us to do the waiting.... so wait.			time.sleep(delay)		if (stage2):				# Breakdown stage2 URL to get params back.		url_parts = urlparse.urlparse(stage2_URL)		#Log(str(url_parts))				params = cgi.parse_qsl(url_parts.query)				# Remove query string arguments we added to form URL to get form URL back.		url_parts =  urlparse.ParseResult(url_parts.scheme, url_parts.netloc, url_parts.path, "", "", "")		#Log(str(url_parts))		form_url = url_parts.geturl()				#Log("*********************************************************************")		#Log("*********************************************************************")		#Log("Params:" + str(params))		#Log("Form URL:" + form_url)				# Submit form.		request = urllib2.Request(url,  urllib.urlencode(params))		request.add_header('User-agent', USER_AGENT)		request.add_header('Referer', form_url)		response = urllib2.urlopen(request)				soup = BeautifulSoup(response.read())		#Log(str(soup))				# Extract out video URL.		final_url = soup.find('div', { 'class':'content-bg'}).center.strong.a['href']		Log(final_url)				ret = []		ret.append(			MediaObject(				parts = [PartObject(key=final_url)],			)		)				return ret		def LogProviderError(msg="", ex=None):	Log("************************** PROVIDER ERROR: " + msg)	return []